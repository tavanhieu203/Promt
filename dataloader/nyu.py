import os
import warnings
import random
import json
import h5py
from PIL import Image
import numpy as np
import torch
import torchvision.transforms as T
import torchvision.transforms.functional as TF
from torch.utils.data import Dataset

warnings.filterwarnings("ignore", category=UserWarning)


class BaseDataset(Dataset):
    def __init__(self, args, mode):
        self.args = args
        self.mode = mode

    def __len__(self):
        raise NotImplementedError

    def __getitem__(self, idx):
        raise NotImplementedError


class ToNumpy:
    def __call__(self, sample):
        return np.array(sample)


class NYU(BaseDataset):
    """
    NYUv2 Depth Completion Dataset (HDF5)
    Sparse depth is generated by uniformly sampling 5% valid pixels
    """

    def __init__(self, args, mode):
        super(NYU, self).__init__(args, mode)

        assert mode in ['train', 'val', 'test']
        self.mode = mode
        self.args = args

        # Image size
        self.height = args.patch_height
        self.width = args.patch_width

        if self.height == 240:
            self.crop_size = (228, 304)
        elif self.height == 480:
            self.crop_size = (456, 608)
        else:
            raise ValueError("Unsupported image height")

        # Camera intrinsics (NYUv2)
        self.K = torch.Tensor([
            5.1885790117450188e+02 / 2.0,
            5.1946961112127485e+02 / 2.0,
            3.2558244941119034e+02 / 2.0 - 8.0,
            2.5373616633400465e+02 / 2.0 - 6.0
        ])

        self.augment = False if mode == 'test' else args.augment

        # Load split file (json)
        with open(args.split_json, 'r') as f:
            self.sample_list = json.load(f)[mode]

    def __len__(self):
        return len(self.sample_list)

    def __getitem__(self, idx):
        # Load HDF5
        path_file = os.path.join(self.args.dir_data,
                                 self.sample_list[idx]['filename'])
        with h5py.File(path_file, 'r') as f:
            rgb_h5 = f['rgb'][:].transpose(1, 2, 0)
            dep_h5 = f['depth'][:]

        rgb = Image.fromarray(rgb_h5, mode='RGB')
        dep = Image.fromarray(dep_h5.astype(np.float32), mode='F')

        # ------------------------------
        # Augmentation (TRAIN only)
        # ------------------------------
        if self.mode == 'train' and self.augment:
            scale_factor = np.random.uniform(1.0, 1.5)
            scale = int(self.height * scale_factor)
            degree = np.random.uniform(-5.0, 5.0)

            if random.random() > 0.5:
                rgb = TF.hflip(rgb)
                dep = TF.hflip(dep)

            rgb = TF.rotate(rgb, degree, resample=Image.NEAREST)
            dep = TF.rotate(dep, degree, resample=Image.NEAREST)

            t_rgb = T.Compose([
                T.Resize(scale),
                T.ColorJitter(0.4, 0.4, 0.4),
                T.CenterCrop(self.crop_size),
                T.ToTensor(),
                T.Normalize((0.485, 0.456, 0.406),
                            (0.229, 0.224, 0.225))
            ])

            t_dep = T.Compose([
                T.Resize(scale),
                T.CenterCrop(self.crop_size),
                ToNumpy(),
                T.ToTensor()
            ])

            rgb = t_rgb(rgb)
            dep = t_dep(dep)
            dep = dep / scale_factor

            K = self.K.clone()
            K[0] *= scale_factor
            K[1] *= scale_factor

        # ------------------------------
        # Validation / Test
        # ------------------------------
        else:
            t_rgb = T.Compose([
                T.Resize(self.height),
                T.CenterCrop(self.crop_size),
                T.ToTensor(),
                T.Normalize((0.485, 0.456, 0.406),
                            (0.229, 0.224, 0.225))
            ])

            t_dep = T.Compose([
                T.Resize(self.height),
                T.CenterCrop(self.crop_size),
                ToNumpy(),
                T.ToTensor()
            ])

            rgb = t_rgb(rgb)
            dep = t_dep(dep)
            K = self.K.clone()

        # ------------------------------
        # Sparse depth (5%)
        # ------------------------------
        dep_sp, num_sample = self.get_sparse_depth(
            dep,
            sparse_ratio=0.05,
            test=(self.mode == 'test')
        )

        output = {
            'rgb': rgb,
            'dep': dep_sp,
            'gt': dep,
            'K': K,
            'num_sample': num_sample
        }

        return output

    def get_sparse_depth(self, dep, sparse_ratio=0.05, test=False):
        """
        Uniformly sample sparse_ratio (e.g., 5%) of valid depth pixels
        """
        channel, height, width = dep.shape
        assert channel == 1

        idx_nnz = torch.nonzero(dep.view(-1) > 1e-4, as_tuple=False)
        num_valid = len(idx_nnz)

        num_sample = int(num_valid * sparse_ratio)

        if test:
            g = torch.Generator()
            g.manual_seed(self.args.sample_seed)
            idx = torch.randperm(num_valid, generator=g)[:num_sample]
        else:
            idx = torch.randperm(num_valid)[:num_sample]

        idx_nnz = idx_nnz[idx]

        mask = torch.zeros(channel * height * width)
        mask[idx_nnz] = 1.0
        mask = mask.view(channel, height, width)

        dep_sp = dep * mask.type_as(dep)

        return dep_sp, num_sample
